{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_nn_grad.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlashOrange/ML_Learning/blob/master/torch_nn_grad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK_nFzXb_-_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Sqcs84a008",
        "colab_type": "text"
      },
      "source": [
        "https://www.bilibili.com/video/av49462224/?p=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpThDpdNAB8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10 # 64个数据，1000维，10维输出(标签)，H为中间层  N实际应当为mini-batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDHg2AlLAJsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建随机的训练数据集\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "#神经网路参数\n",
        "w1 = torch.randn(D_in, H, requires_grad=True) #导入中间层\n",
        "w2 = torch.randn(H, D_out, requires_grad=True) #转换到输出"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdYQPXVxANiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-6\n",
        "#迭代500次的梯度下降\n",
        "for it in range(500):\n",
        "    #step 1:forward pass\n",
        "    #生成 N*H矩阵\n",
        "    #用relu函数激活中间层\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2) # 预测，生成 N*D_out的矩阵，这是一张graph\n",
        "    \n",
        "    #step 2:  MSE loss\n",
        "    loss = (y_pred - y).pow(2).sum() \n",
        "    print(it, loss.item())\n",
        "    \n",
        "    #step3: Backward pass 链式求导\n",
        "    ## compute grad d(loss)/d(w1)\n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    #step4: update w1,w2\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad # 这也是计算图\n",
        "        w2 -= learning_rate * w2.grad\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzeL0NGIASGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czi1M5QxFg3x",
        "colab_type": "text"
      },
      "source": [
        "使用Torch.nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENBERkHFk4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10 # 64个数据，1000维，10维输出(标签)，H为中间层  N实际应当为mini-batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9UUI1EaF_qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建随机的训练数据集\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "#建立神经网络模型\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D_in, H, bias=False), #和上面不同，这里是包含bias的\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, D_out)\n",
        ")\n",
        "\n",
        "#正规化\n",
        "torch.nn.init.normal_(model[0].weight)\n",
        "torch.nn.init.normal_(model[2].weight)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "#定义损失函数\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "#迭代500次的梯度下降\n",
        "for it in range(500):\n",
        "    #step 1:forward pass\n",
        "    #生成 N*H矩阵\n",
        "    #用relu函数激活中间层\n",
        "    y_pred = model(x)  # model.forward()\n",
        "    \n",
        "    #step 2:  MSE loss\n",
        "    loss = loss_fn(y_pred,y)\n",
        "    print(it, loss.item())\n",
        "    #step3: Backward pass 链式求导\n",
        "    ## compute grad d(loss)/d(w1)\n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    #step4: update w1,w2\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "    model.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJggCP7mSdyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "41429bfb-572a-46ff-90c4-8f08f68de908"
      },
      "source": [
        "model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=1000, out_features=100, bias=False)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_evtBkWT46E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model[0].weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgZNK5guT7JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ukkf5zXZwYP",
        "colab_type": "text"
      },
      "source": [
        "### 更加自动化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUwWIXkeZzqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgsuNUvFZ2Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10 # 64个数据，1000维，10维输出(标签)，H为中间层  N实际应当为mini-batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgcEQSt0Z44s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建随机的训练数据集\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "#建立神经网络模型\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D_in, H, bias=False), #和上面不同，这里是包含bias的\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, D_out)\n",
        ")\n",
        "\n",
        "#正规化初始w(这里如果正规化，效果会不好)\n",
        "#torch.nn.init.normal_(model[0].weight)\n",
        "#torch.nn.init.normal_(model[2].weight)\n",
        "\n",
        "learning_rate = 1e-4 #Adam优化器的参数一般在 1e-4 - 1e-3之间\n",
        "\n",
        "#定义损失函数\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "#定义优化器\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#迭代500次的梯度下降\n",
        "for it in range(500):\n",
        "    #step 1:forward pass\n",
        "    #生成 N*H矩阵\n",
        "    #用relu函数激活中间层\n",
        "    y_pred = model(x)  # model.forward()\n",
        "    \n",
        "    #step 2:  MSE loss\n",
        "    loss = loss_fn(y_pred,y)\n",
        "    print(it, loss.item())\n",
        "    \n",
        "    #step3 :清空Gard\n",
        "    optimizer.zero_grad()\n",
        "    #step4: Backward pass \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    #step5: update w\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwI0dJ4Wb1Wj",
        "colab_type": "text"
      },
      "source": [
        "##  自定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_nhn13nZ9M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class TwoLayerNet(torch.nn.Module): #从nn.model继承\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
        "    self.linear2 = torch.nn.Linear(H, D_out)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs3mUpZHhX46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10 # 64个数据，1000维，10维输出(标签)，H为中间层  N实际应当为mini-batch\n",
        "model = TwoLayerNet(D_in, H, D_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VceWXE3kksX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#正规化初始w(这里如果正规化，效果会不好)\n",
        "#torch.nn.init.normal_(model[0].weight)\n",
        "#torch.nn.init.normal_(model[2].weight)\n",
        "\n",
        "learning_rate = 1e-4 #Adam优化器的参数一般在 1e-4 - 1e-3之间\n",
        "\n",
        "#定义损失函数\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "#定义优化器\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#迭代500次的梯度下降\n",
        "for it in range(500):\n",
        "    #step 1:forward pass\n",
        "    #生成 N*H矩阵\n",
        "    #用relu函数激活中间层\n",
        "    y_pred = model(x)  # model.forward()\n",
        "    \n",
        "    #step 2:  MSE loss\n",
        "    loss = loss_fn(y_pred,y)\n",
        "    print(it, loss.item())\n",
        "    \n",
        "    #step3 :清空Gard\n",
        "    optimizer.zero_grad()\n",
        "    #step4: Backward pass \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    #step5: update w\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW078GtnlXba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}