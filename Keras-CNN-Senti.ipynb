{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba \n",
    "\n",
    "from keras.layers import Dense, Input, Flatten,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fneg = open('neg.txt','r',encoding='gb18030')\n",
    "fpos = open('pos.txt','r',encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "pos = []\n",
    "for line in fneg:\n",
    "    neg.append(line)\n",
    "for line in fpos:\n",
    "    pos.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame(neg)\n",
    "pos_df = pd.DataFrame(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>早餐比较差，别的都还好啦。离香洲汽车站满近的，步行不到10分钟也就到了。这点满方便的。\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>晚上到的深圳机场，打的到酒店半个小时左右，120块的样子.酒店CHECK IN 还算快.但是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>房间整体感觉十分干净，环境也比较清静，虽然临街一边的客房要相对吵一些，但是隔音做的不错\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>八月初入住两天,感觉极好.幽雅的庭院式格局与众多的现代高楼酒店相比,给人一种苏州园林式的恬静...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>大连很不错的酒店了，不过差点和南山花园那个搞错0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "4995      早餐比较差，别的都还好啦。离香洲汽车站满近的，步行不到10分钟也就到了。这点满方便的。\\n\n",
       "4996  晚上到的深圳机场，打的到酒店半个小时左右，120块的样子.酒店CHECK IN 还算快.但是...\n",
       "4997      房间整体感觉十分干净，环境也比较清静，虽然临街一边的客房要相对吵一些，但是隔音做的不错\\n\n",
       "4998  八月初入住两天,感觉极好.幽雅的庭院式格局与众多的现代高楼酒店相比,给人一种苏州园林式的恬静...\n",
       "4999                         大连很不错的酒店了，不过差点和南山花园那个搞错0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并语料\n",
    "\n",
    "pn = pd.concat([neg_df, pos_df], ignore_index=True)\n",
    "neg_len = len(neg_df)\n",
    "pos_len = len(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0728 08:21:20.159217  7936 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Andy\\AppData\\Local\\Temp\\jieba.cache\n",
      "I0728 08:21:23.505533  7936 __init__.py:145] Dumping model to file cache C:\\Users\\Andy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.620 seconds.\n",
      "I0728 08:21:23.783891  7936 __init__.py:163] Loading model cost 3.620 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "I0728 08:21:23.787896  7936 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#定义分词函数\n",
    "\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "pn['words'] = pn[0].apply(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>专程从成都来绵阳看第四届科博会，在万达广场吃了晚饭都快九点了，携程上匆忙订了王子大酒店冲着它...</td>\n",
       "      <td>[专程, 从, 成都, 来, 绵阳, 看, 第四届, 科博会, ，, 在, 万达, 广场, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我本人办的有锦江之星的会员卡，相对来说锦江住的比较多，石家庄的锦江我只住过平安大街店，感觉非...</td>\n",
       "      <td>[我, 本人, 办, 的, 有, 锦江, 之星, 的, 会员卡, ，, 相对来说, 锦江, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>可能是以上海地区而言不算贵吧,但是一切都乏善可陈,就是说不出甚么特别不好的,但是也说不出一点...</td>\n",
       "      <td>[可能, 是, 以, 上海地区, 而言, 不算, 贵, 吧, ,, 但是, 一切, 都, 乏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>建议携程取消推荐该酒店,环境很差,楼下有迪厅,不仅有噪音,整个房间都在颤动,根本无法入睡!另...</td>\n",
       "      <td>[建议, 携程, 取消, 推荐, 该, 酒店, ,, 环境, 很差, ,, 楼下, 有, 迪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>周围环境差,房间设施陈旧.房间小.服务员懒散.去餐厅很不方便,路线复杂.早餐贵,而且没有什么...</td>\n",
       "      <td>[周围环境, 差, ,, 房间, 设施, 陈旧, ., 房间, 小, ., 服务员, 懒散,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  专程从成都来绵阳看第四届科博会，在万达广场吃了晚饭都快九点了，携程上匆忙订了王子大酒店冲着它...   \n",
       "1  我本人办的有锦江之星的会员卡，相对来说锦江住的比较多，石家庄的锦江我只住过平安大街店，感觉非...   \n",
       "2  可能是以上海地区而言不算贵吧,但是一切都乏善可陈,就是说不出甚么特别不好的,但是也说不出一点...   \n",
       "3  建议携程取消推荐该酒店,环境很差,楼下有迪厅,不仅有噪音,整个房间都在颤动,根本无法入睡!另...   \n",
       "4  周围环境差,房间设施陈旧.房间小.服务员懒散.去餐厅很不方便,路线复杂.早餐贵,而且没有什么...   \n",
       "\n",
       "                                               words  \n",
       "0  [专程, 从, 成都, 来, 绵阳, 看, 第四届, 科博会, ，, 在, 万达, 广场, ...  \n",
       "1  [我, 本人, 办, 的, 有, 锦江, 之星, 的, 会员卡, ，, 相对来说, 锦江, ...  \n",
       "2  [可能, 是, 以, 上海地区, 而言, 不算, 贵, 吧, ,, 但是, 一切, 都, 乏...  \n",
       "3  [建议, 携程, 取消, 推荐, 该, 酒店, ,, 环境, 很差, ,, 楼下, 有, 迪...  \n",
       "4  [周围环境, 差, ,, 房间, 设施, 陈旧, ., 房间, 小, ., 服务员, 懒散,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#统计一行最多有多少个单词\n",
    "\n",
    "max_document_length = max([len(x) for x in pn['words']])\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将一行最大单词数设为1000\n",
    "max_document_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(x) for x in pn['words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'专程 从 成都 来 绵阳 看 第四届 科博会 ， 在 万达 广场 吃 了 晚饭 都 快 九点 了 ， 携程 上 匆忙 订 了 王子 大酒店 冲着 它 是 四星级 ， 门脸 很 阔气 大堂 够 大气 ， 进 了 客房 略微 失望 ， 洗个 澡 吓死 宝宝 ， 看 图 说话 。 是 我 图 便宜 就 选到 了 250 一晚 的 标间 ， 怪 自己 了 ， 这 酒店 评级 标准 咋 能 这样 呢 ， 老 四星 也 太 追求 环保 了 吧 ， 该换 的 不换 该 清理 的 不 清理 这是 评级 标准 吗 ？   想 住 绵阳 四星 的 朋友 看 过来 ， 要 住 就 住 他家 五百 八百 的 房 吧 ， 二百五十 多 的 就 别来 了 ， 去汉庭 7 天 算了   还有 停车 过夜 ， 前台 美女 说 ： 我们 星级 酒店 没有 免费 停车 的 。 。 。   此处 省略 一万 字 \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实例化分词器\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "#建立词典\n",
    "tokenizer.fit_on_texts(texts)\n",
    "#将词频和序号对应\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "#将序列设置为1000的长度，超过1000的部分舍弃，不足的补0\n",
    "sequences = pad_sequences(sequences, maxlen=1000, padding='post')\n",
    "#sequences = np.array(sequences)\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8369,   137,  1243, ...,     0,     0,     0],\n",
       "       [    9,   494,   839, ...,     0,     0,     0],\n",
       "       [  183,     6,   567, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    7,   321,    33, ...,     0,     0,     0],\n",
       "       [10142, 11861,    24, ...,     0,     0,     0],\n",
       "       [ 1401,     8,    17, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#词对应编号的词典\n",
    "\n",
    "dict_text = tokenizer.word_index\n",
    "dict_text['天']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8369,   137,  1243,    93,  4981,   150, 14895, 14896,     1,\n",
       "          12,  4982,   783,   111,     4,  1683,    13,   535,  1991,\n",
       "           4,     1,    47,    59,  2156,   164,     4,  7050,   603,\n",
       "        2526,   445,     6,   374,     1,  4983,     8,  8370,   123,\n",
       "         585,  2223,     1,   490,     4,   220,  3973,   362,     1,\n",
       "        2527,  1321,  4984,  2086,     1,   150,  2284,   838,     3,\n",
       "           6,     9,  2284,   258,    22, 14897,     4,  4242,   454,\n",
       "           2,   377,     1,  2224,   138,     4,     1,    80,     5,\n",
       "        3708,   205,  2034,   100,   122,   324,     1,   298,   268,\n",
       "          10,    82,  5481,  2985,     4,   115,     1,  5482,     2,\n",
       "        3974,   166,  1432,     2,    15,  1432,   472,  3708,   205,\n",
       "         340,    85,   190,    21,  4981,   268,     2,   167,   150,\n",
       "         550,     1,    38,    21,    22,    21,  2986, 10471,  6170,\n",
       "           2,    64,   115,     1, 14898,    89,     2,    22, 10472,\n",
       "           4,     1, 10473,   204,   346,  1529,    79,   530,  2087,\n",
       "           1,    32,  3498,    27,    39,    29,   308,     5,    18,\n",
       "         112,   530,     2,     3,     3,     3,  4572,  8371, 14899,\n",
       "         694,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里的x是句子中每个分词对应的word_index的列表\n",
    "\n",
    "#定义标签\n",
    "positive_labels = [[0, 1] for _ in range(pos_len)]\n",
    "negative_labels = [[1, 0] for _ in range(neg_len)]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "#打乱数据\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = sequences[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "#数据集切分\n",
    "test_sample_index = -1 * int(0.15 * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "#模型输入\n",
    "sequence_input = Input(shape=(1000,))\n",
    "#Embedding层，30000表示30000个词，每个词对应向量128维，序列长度1000\n",
    "embedding_layer = Embedding(\n",
    "    30000,  #生成一个30000*128的矩阵\n",
    "    128, \n",
    "    input_length=1000\n",
    ")\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#卷积核大小为3\n",
    "\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding_sequences)\n",
    "cnn1 = MaxPooling1D(pool_size=5)(cnn1)\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(cnn1)\n",
    "cnn1 = MaxPooling1D(pool_size=5)(cnn1)\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(cnn1)\n",
    "cnn1 = MaxPooling1D(pool_size=37)(cnn1)\n",
    "cnn1 = Flatten()(cnn1)\n",
    "                                                            \n",
    "#卷积核大小为4\n",
    "\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_sequences)\n",
    "cnn2 = MaxPooling1D(pool_size=5)(cnn2)\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(cnn2)\n",
    "cnn2 = MaxPooling1D(pool_size=5)(cnn2)\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(cnn2)\n",
    "cnn2 = MaxPooling1D(pool_size=36)(cnn2)\n",
    "cnn2 = Flatten()(cnn2) \n",
    "                                                            \n",
    "#卷积核大小为5\n",
    "\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(embedding_sequences)\n",
    "cnn3 = MaxPooling1D(pool_size=5)(cnn3)\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(cnn3)\n",
    "cnn3 = MaxPooling1D(pool_size=5)(cnn3)\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(cnn3)\n",
    "cnn3 = MaxPooling1D(pool_size=35)(cnn3)\n",
    "cnn3 = Flatten()(cnn3)\n",
    "\n",
    "#合并\n",
    "merge = concatenate([cnn1, cnn2, cnn3], axis=1)\n",
    "#全连接层\n",
    "\n",
    "x = Dense(128, activation='relu')(merge)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1000, 128)    3840000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 998, 32)      12320       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 997, 32)      16416       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 996, 32)      20512       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 199, 32)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 199, 32)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 199, 32)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 197, 32)      3104        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 196, 32)      4128        max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 195, 32)      5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 39, 32)       0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 39, 32)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 39, 32)       0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 37, 32)       3104        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 36, 32)       4128        max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 35, 32)       5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 1, 32)        0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 1, 32)        0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 1, 32)        0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 32)           0           max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 32)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 32)           0           max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 96)           0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          12416       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,926,690\n",
      "Trainable params: 3,926,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 16:13:45.152997  7936 deprecation_wrapper.py:119] From C:\\Conda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 16:13:45.219060  7936 deprecation_wrapper.py:119] From C:\\Conda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#编译和训练\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy',\n",
    "    #loss = 'mse',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 16:14:14.525673  7936 deprecation.py:323] From C:\\Conda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8500/8500 [==============================] - 123s 15ms/step - loss: 0.4193 - acc: 0.7884\n",
      "Epoch 2/10\n",
      "8500/8500 [==============================] - 119s 14ms/step - loss: 0.1506 - acc: 0.9455\n",
      "Epoch 3/10\n",
      "8500/8500 [==============================] - 117s 14ms/step - loss: 0.0491 - acc: 0.9859\n",
      "Epoch 4/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0186 - acc: 0.9951\n",
      "Epoch 5/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0139 - acc: 0.9960\n",
      "Epoch 6/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0052 - acc: 0.9986\n",
      "Epoch 7/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 8/10\n",
      "8500/8500 [==============================] - 117s 14ms/step - loss: 7.2478e-04 - acc: 0.9999\n",
      "Epoch 9/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0218 - acc: 0.9936\n",
      "Epoch 10/10\n",
      "8500/8500 [==============================] - 116s 14ms/step - loss: 0.0055 - acc: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x89bf428b00>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测函数\n",
    "\n",
    "def predict(text):\n",
    "    cw = list(jieba.cut(text))\n",
    "    word_id = []\n",
    "    for word in cw:\n",
    "        try:\n",
    "            temp = dict_text(word)\n",
    "            word_id.append(temp)\n",
    "        except:\n",
    "            word_id.append(0)\n",
    "    word_id = np.array(word_id)\n",
    "    word_id = word_id[np.newaxis,:]\n",
    "    sequences = pad_sequences(word_id,maxlen=1000,padding='post')\n",
    "    print(model.predict(sequences))\n",
    "    result = np.argmax(model.predict(sequences))\n",
    "    if result == 1:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Positive\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01159209 0.9884079 ]]\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "predict(\"这里的天气真是糟透了，店员脑子瓦特了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500/8500 [==============================] - 15s 2ms/step\n",
      "Loss: 0.00023811632527555475\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
